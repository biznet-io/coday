# Description of the current project, it should contain the same high-level information and rules about the project anyone of the team should know.
#
# This is used as a system instruction, hence it is sent at the very beginning of any new thread, whatever the assistant involved is, so other assistants will also have access to this message as part of the openai thread (or hopefully context if truncated). If using the default Coday assistant (that is kept quite generic and close to default LLM), the more detailed and broad the description, the more Coday's responses are relevant (by a very wide margin).
#
# Recommendations: write it in markdown as you would write human-intended documentation
description: |
  ## Project description
  
  This project is a chat AI assistant running by calling an online LLM, for now only ChatGPT from OpenAI.
  
  The interface is kept very simple, presently only through a terminal, but could later run as a local application with dedicated GUI or as a web page or an IDE extension.
  The goal is to integrate the LLM with as much tools as needed to increase its efficiency and relevance in helping users or even handling tasks autonomously. 
  
  
  ## Architecture
  
  At the core of Coday is a interactive loop that will take the user input and pass it to the handler loop, and come back waiting for user input when done. It also handle the `oneshot` non-interactive mode where the initially given user commands (same as prompts) are queued and handled until exhaustion and the program ends.
  
  The handler loop will exhaust the commands taken from the given context by finding the very first handler that can accept each command, let it handle and continue with the next command of the queue. As handlers can add commands to the queue (first in - first out), a single high-level user command can generate in the end a large number of low-level commands.
  If no matching handler is found, the command is re-written to be re-directed to the AI.
  
  
  ### Handlers
  
  Logic for each command the user can call is wrapped in command handlers that can be simple or nested for complex commands exposing sub-commands.
  A command is a string that starts with a precise `commandWord` specific to the handler, and possibly some other text beyond.
  Handlers are asked first if they accept the command, and they should not do anything else than checking.
  Then, the found handler is given the command to handle, to de-construct it and do the expected work with it.
  When the command is invalid, handlers are expected to use the given interactor to signal the invalidity to the user, and possibly display a help or usage text.
  Handlers are in `./src/handler/` folder.
  
  
  ### Context
  
  The command context keeps track of various data dictating the behavior of parts of the application : the project and nested configuration about its path, integrations scripts and assistants. This project config data structure is the merge of the user's project settings, and the project's shared configuration.
  The command context also contains the command queue that can be enriched through handlers.
  
  
  ### Configuration files
  
  The program uses a configuration file in user home directory under '~/.coday/config.json', that lists the projects and their non-sharable and user-related configuration (API key notably).
  Projects should also contain a `coday.yaml` file that can be stored in version control system and shared on the repository.
  A project is a directory whose files should be readable and editable and where a 'coday.yaml' configuration file reside (can be nested) to add some customizations. As such, the 'config.json' is entirely private to the user and 'coday.yaml' is exposed to all users of the project.
  
  
  ### Integrations
  
  An integration designate the configuration of an access to an API (can be anything reachable from local tools to web-based http APIs), and by extension includes the depending parts of this project (handlers, tools & functions).
  If an integration is not defined (even if no parameters are required as for GIT), the corresponding handlers should not be available to the user, and corresponding tools should not be available to the AI/LLM.
  Integration are in `./src/integration/` folder.
  
  
  ### Services
  
  The services in this project provide core functionalities that support the configuration, integration, and management of the project. They handle tasks such as managing user and project configurations, setting up and retrieving integration details, initializing or loading project descriptions, and saving or listing threads related to the project. These services ensure that the essential data and settings are properly maintained and accessible throughout the application's lifecycle.
  Services are in `./src/service` folder. 
  
  
  ## AI/LLM integration
  
  The `openai-handler.ts` that uses `openai-client.ts` wraps the complexity around the library openai and serves as a default handler for commands not picked by any other handler. 
  This client leverages the ability to submit function declaration for LLM to call, hence the need on our part to run the function with the given arguments and return the answer.
  These functions can be provided to the LLM depending on the user configuration and the project configuration.
  
  
  ## Code conventions:
  
  - inject in constructors
  - no ';' on line ends
  - prefer 1 file for 1 class or function or type
  
  
  ## Git process
  
  All developments should not happen on 'master' branch, but on a dedicated and named by type of dev (feat, bugfix, other...), user name, and a title .
  Ex: `bugfix/johndoe/find-by-text-errors`
  
  Commit only after compilation success.
  Every commit message should have a title summarizing the change (do not repeat the branch name), and then a description with more details with an empty line between title and description.
  The title should be less than 50 characters long, and the description should be wrapped at 72 characters.

mandatoryDocs:
  - ./doc/INTEGRATIONS.md
  - ./src/model/command-context.ts
  - ./src/coday.ts
  - ./src/handler-looper.ts
  - ./src/model/command.handler.ts

optionalDocs:
  - path: ./doc/PROJECT_CONFIGURATION.md
    description: How the project configuration works
  - path: ./src/model/nested.handler.ts
    description: Handler for complex commands that host sub-commands, each to be taken care of by another handler

# Scripts are AssistantTools declared to openai on each call (for now), so always available.
# Scripts that take parameters should have:
#   - mandatory 'parametersDescription' string attribute: be very explicit about what it is and should be.
#   - optional 'PARAMETERS' string value in the command (to replace at runtime by the LLM input matching the 'parametersDescription'). If absent, the LLM input parameters are added as suffix to the command.
# Always be explicit in name and description, commands are run from the project root directory only !
#
# example:
#   say-something:
#    description: Just says something, serves as demo of a project script with parameters
#    command: echo "PARAMETERS"
#    parametersDescription: text that will be displayed to the user

scripts:
  compile:
    description: compile the typescript project to raise any issue on code correctness, it does not run the tests.
    command: yarn run tsc
  open-file-in-ide-for-user:
    description: opens the file(s) in the user IDE.
    command: idea
    parametersDescription: the local paths to the files to open, separated by a space.


# The prompts section allows you to define custom sequences of commands, known as prompt chains, 
# that the system can execute in order. Each prompt chain is identified by a unique key and 
# consists of a description, a list of commands, and optionally, required integrations.
#
# Parameters:
#   - description: A brief explanation of what the prompt chain does.
#   - commands: An array of commands that will be executed in sequence. The placeholder 
#     keyword `PROMPT` in these commands will be replaced with the user's input.
#   - requiredIntegrations: (Optional) A list of integrations that must be available for 
#     the prompt chain to function.
#
# Recommendations:
#   - Use clear and descriptive keys for each prompt chain to make them easily identifiable.
#   - Provide a detailed description to help users understand the purpose and behavior of the 
#     prompt chain.
#   - Ensure that the commands are valid and correctly formatted, especially when using the 
#     `PROMPT` keyword.
#   - Specify any required integrations to make the prompt available.

prompts:
  say-hello:
    description: |
      a dummy prompt chain for demo, the `PROMPT` value comes from the sub-command, ex: `say-hello answer with banana` => PROMPT = `answer with banana`. PROMPT can be used in several commands.
    commands:
      - "@ hello, PROMPT"
      - "@ how are you ?"
    requiredIntegrations:
      - "GIT"

# Multiple specialized assistants can be declared on a project: they "know" each-other by their description and can cooperate on a same thread if asked to. They can also be called directly.
# Of the parameters:
#   - name: should be the full name of the assistant as per the LLM provider. A matching will be done with the beginning of the name for easier interactions, but better have the full name here.
#   - description: what the other assistants should know of this assistant. All assistants are declared as system instruction at the beginning of any new thread (and explained how to call one another), hence everybody knows everybody.
#   - systemInstructions: used for automatic assistant creation only. If absent, the assistant will not be created when called and missing from API. If present and assistant called is missing, it will be automatically created with this system instruction and nothing else.
#   - temperature: refer to the temperature of the openai API, 0 = ~deterministic, 1 = normal, 2 = extremely imaginative
#
# NOTE: It is possible to use customized assistants (with files or fine-tuning or whatever: call them by name) but not create them through Coday.
assistants:
  - name: aitutor_coday
    description: counselor in LLM use to create AI agents
    systemInstructions: |
      You are a thoughtful expert in the use of ChatGPT LLMs.
      
      You are able to counsel on the way to use this technology and work around its limitations. You can provide information on Openai api and features of the platform.
      
      While future LLMs might improve enough to reach artificial general intelligence or beyond, you know that today the real opportunity to have an AI agentic system is to have several specialized LLMs to collaborate on a given task and following a workflow or broad rules of discussion, collaboration and cross-checking.
      
      Your endgoal is to see this project provide work value on par of humans and greatly augment them.

    temperature: 1.2
  - name: dev_coday
    description: expert software developer knowledgeable on the project.
    systemInstructions: |
      You are a friendly expert software developer in typescript and nodejs, aiming at producing flexible yet simple code.
      
      You always decompose a task into simple atomic steps and follow a careful workflow and enjoy working with your colleagues. Testing is important to you, and to be included in development if the task or context allows it.
      
      Your endgoal is being fully replaceable as the code you wrote is easy to understand and highly evolutive yet robust.
    temperature: 0.5
  - name: ux_coday
    description: user experience enthusiast wanting to push the project forward
    systemInstructions: |
      You are an enthusiastic user experience expert, aiming for the project providing the best experience for the user. 
      
      You accept some early limitations in working only through a limited terminal interface, but want to improve the flow, speed and capabilities of the program in providing an ai agent-like service. You target some more user-friendly interfaces like a web page, an IDE plugin or even voice interaction. 
      
      Your endgoal is integrating an AI agent into a human software development project, through the existing tools (task management, slack, continuous integration platform, ...), as a local program or a remote instance.
    temperature: 0.9
